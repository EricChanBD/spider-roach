{"name":"Spider-roach","tagline":"一个分布式定向抓取集群的简单实现。","body":"声明\r\n========\r\n\r\n本人自知精力与能力有限，欢迎志同道合之士送上您宝贵的建议与Patch!\r\n\r\n\r\n\r\n关于\r\n========\r\n\r\n一个分布式定向抓取集群的简单实现。\r\n\r\nReference:\r\n    程序架构参考淘宝官博：www.searchtb.com/2011/07/快速构建实时抓取集群.html\r\n\r\n目前实现功能\r\n-------------\r\n\r\n1. 多线程下载,线程数可配置。\r\n2. 无需修改代码，按照规则添加配置就可以完成页面抽取、入库。\r\n3. 利用Redis的list作为抓取队列，zset作为已抓取集合。\r\n4. 支持分布式部署多个爬虫，Redis作为核心，mysql为存储,当然redis/mysql自身拥有各自的扩展方案。\r\n\r\n\r\nTODO List\r\n-------------\r\n\r\n1. KISS: Keep it simple & stupid!\r\n2. Supports cookies,and authentication.\r\n3. Write information into files (using protobuf??).\r\n\r\n\r\n\r\nINSTALL\r\n========\r\n\r\n确认安装Python2.7及依赖库: \r\n        \r\n        MySQLdb: http://sourceforge.net/projects/mysql-python/\r\n        \r\n        redis: https://pypi.python.org/pypi/redis/\r\n        \r\n        lxml: http://lxml.de/\r\n        \r\n下载源码包:\r\n\r\n        git clone https://github.com/agathewiky/spider-roach.git\r\n\r\n\r\nHow?\r\n========\r\n\r\n\r\nscheduler.py \r\n-------------\r\n\r\n定义自己的爬虫,实现爬虫的调度算法，并将pipeline中负责解析的类注册到爬虫中;\r\n继承BaseSpider基类即可实现一个自己的爬虫，然后重写 Rules函数，定义自己的爬行策略。\r\n\r\n:: \r\n\r\n    class spider_name(BaseSpider):\r\n        def Rules(self):\r\n            #linkbase\r\n            linkbase = getRedis()\r\n            url_list = DQueue(linkbase,'url_list')\r\n            url_set = Record(linkbase, 'crawled_set')\r\n            base.url_maps = get_Maps('./maps.cfg')\r\n            list = {\r\n                'url':url_list,\r\n                'url_set':url_set,\r\n            }\r\n            self.AddRules(list, 'Parse_url', 'url', 10)\r\n    \r\n        def scheduling(self):\r\n            \"\"\"重写scheduling,实现自己的调度策略\"\"\"\r\n            start_url = 'http://venue.damai.cn/search.aspx'\r\n            url_list.push(start_url)\r\n            while 1:\r\n                #do something\r\n                time.sleep(5)\r\n    \r\n\r\nmaps.cfg\r\n-------------\r\n\r\n配置待抓取页面相应的抽取规则\r\n详细例子参见maps.cfg\r\n\r\n::\r\n\r\n    {\r\n        \"http://venue.damai.cn/search.aspx\":{\r\n            \"info\":\"抓取XX网场馆列表页页\",\r\n            \"pre_url\":\"http://venue.damai.cn\",#抽取出的link添加默认前缀\r\n            \"link_xpath\":[\r\n                \"//div[@class='pagination']/a[@class='next']/@href\", #下一页\r\n                \"//span[@class='type']/h3/a/@href\", #详情页\r\n                ],\r\n         },\r\n        \"http://venue.damai.cn/venue\":{\r\n            \"info\":\"抓取XX网场馆详情页\",\r\n            \"table\":\"pastime\", #mysql表名\r\n            \"page_xpath\":{\r\n                \"name\":\"//div[@class='site_guide']/a[3]/text()\", #与mysql表字段名必须保持一致\r\n                \"image\":\"//div[@class='venueDetal']/p/img[@class='img']/@src\",\r\n                \"detail\":\"//div[@class='info']/div/text()\",\r\n                }\r\n         },\r\n    }\r\n\r\n\r\nsettings.py\r\n-------------\r\n\r\n配置Redis,mysql的连接参数\r\n配置maps.cfg路径位置\r\n\r\n\r\nRUN\r\n========\r\n\r\n首先确认redis和mysql服务是否已启动并可用，然后执行：\r\n\r\n::\r\n\r\n    ./crawl spider_name\r\n    options:\r\n        -d ./logs 可将输出写入指定文件夹的日志中\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}